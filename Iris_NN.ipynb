{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout for this notebook\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Pre-Process the dataset\n",
    "2. Defining the architectiure \n",
    "3. Initialization\n",
    "4. Forward prop \n",
    "5. Compute loss \n",
    "6. Backward prop\n",
    "7. Update parameters\n",
    "8. Repeat steps 4-7\n",
    "9. Predict\n",
    "\n",
    "\n",
    "Architecture:\n",
    "4-4-1\n",
    "\"\"\"\n",
    "\n",
    "print(\"Layout for this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "OHE = OneHotEncoder(sparse_output=False)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "iris = fetch_ucirepo(id=53) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = iris.data.features \n",
    "y = iris.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(iris.metadata) \n",
    "  \n",
    "# variable information \n",
    "# print(iris.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Normalize X\\n2. One hot encode y\\n3. Create the train and test split\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Normalize X\n",
    "2. One hot encode y\n",
    "3. Create the train and test split\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.03205722, -1.3412724 , -1.31297673],\n",
       "       [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673],\n",
       "       [-1.38535265,  0.33784833, -1.39813811, -1.31297673],\n",
       "       [-1.50652052,  0.10644536, -1.2844067 , -1.31297673],\n",
       "       [-1.02184904,  1.26346019, -1.3412724 , -1.31297673]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Normalize X\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. One hot encode y\n",
    "\n",
    "y = np.array(y['class'])\n",
    "y = y.reshape(-1, 1)\n",
    "y = OHE.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "# 80-20\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Defining the architecture\n",
    "\n",
    "n_x = 4 # no. of input variables\n",
    "n_h = 4 # no. of hidden units\n",
    "n_y = 1 # no. of output units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialization\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the softmax function\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Forward prop\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)  \n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compute cost\n",
    "\n",
    "def compute_cost(Y, A2):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # print(\"Y shape =\", Y.shape)\n",
    "    # print(\"A2 shape =\", A2.T.shape)\n",
    "    # sample_product = np.dot(Y, np.log(A2).T)\n",
    "    # print(\"Shape after dot product =\", np.sum(sample_product, axis=0))\n",
    "    # print(\"Regular multiplication =\", np.sum(np.multiply(Y, np.log(A2))))\n",
    "    # print(\"Shape of regular product\", np.sum(np.multiply(Y, np.log(A2))))\n",
    "    cost = -(1/m) * np.sum(np.sum(np.multiply(Y, np.log(A2))))\n",
    "    # cost = -(1/m) * np.sum(np.dot(Y, np.log(A2).T))\n",
    "    \n",
    "    # cost = float(np.squeeze(cost))\n",
    "    \n",
    "    # Manually calculate a sample to see why cost can't be calculated using dot product\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Backward prop\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Update parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Repeat steps 3-6\n",
    "\n",
    "def nn_model(X, Y, n_h, learning_rate, num_iterations = 10000, print_cost=False):\n",
    "    n_x = 4\n",
    "    n_y = 3\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]  \n",
    "    \n",
    "    for i in range(0, num_iterations+1):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(Y, A2)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"The cost at iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost at iteration 0: 1.098627\n",
      "The cost at iteration 1000: 0.042644\n",
      "The cost at iteration 2000: 0.037531\n",
      "The cost at iteration 3000: 0.033430\n",
      "The cost at iteration 4000: 0.029452\n",
      "The cost at iteration 5000: 0.027663\n",
      "The cost at iteration 6000: 0.026579\n",
      "The cost at iteration 7000: 0.025805\n",
      "The cost at iteration 8000: 0.025208\n",
      "The cost at iteration 9000: 0.024726\n",
      "The cost at iteration 10000: 0.024327\n",
      "W1 = [[-1.42387912 -0.68658443  0.73778788 -6.82272974]\n",
      " [-2.265572    0.05452731  7.98809635  0.53323523]\n",
      " [-0.77271216  0.6228368  -1.50330811 -1.32618962]\n",
      " [-1.14269875 -5.46487212  3.92966822  1.48732248]]\n",
      "b1 = [[ 5.42180995]\n",
      " [-3.55977204]\n",
      " [-1.33000135]\n",
      " [-3.30238793]]\n",
      "W2 = [[ 1.89846014 -2.4741868   5.51984091 -3.06133006]\n",
      " [ 3.96673306 -2.88455483 -4.41426539 -2.11828805]\n",
      " [-5.87975426  5.36308956 -1.14012711  5.18148841]]\n",
      "b2 = [[ 0.7523491 ]\n",
      " [ 0.02336949]\n",
      " [-0.77571859]]\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X_train.T, y_train.T, 4, 1.3, num_iterations=10000, print_cost=True)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction & Post training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Predict\n",
    "\n",
    "def predict(parameters, X, y):\n",
    "    # Get predictions by using the final weights on a forward pass\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = np.argmax(A2, axis=0) \n",
    "\n",
    "    # Get the true values\n",
    "    y_values = np.argmax(y, axis=1)\n",
    "    \n",
    "    # Check how many of them are same and compute the average \n",
    "    accuracy = np.mean(predictions == y_values) * 100\n",
    "    \n",
    "    print(f\"{accuracy:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.17% accuracy\n"
     ]
    }
   ],
   "source": [
    "predict(parameters, X_train.T, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% accuracy\n"
     ]
    }
   ],
   "source": [
    "predict(parameters, X_test.T, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
